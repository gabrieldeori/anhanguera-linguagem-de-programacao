import tensorflow as tf
import gymnasium as gym


# Ambiente CartPole do Gym
env = gym.make('CartPole-v1')


# Modelo Simples para Aprendizado por Reforço
model_reinforcement = tf.keras.Sequential([
    tf.keras.layers.Dense(24, activation='relu', input_shape=(env.observation_space.shape[0],)),
    tf.keras.layers.Dense(env.action_space.n, activation='linear')
])


model_reinforcement.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')


# Treinamento por Reforço (exemplo fictício)
max_episodes = 1000  # Defina o número máximo de episódios
for episode in range(max_episodes):
    state, _ = env.reset()  # Captura o estado e ignora o segundo valor retornado
    done = False

    while not done:
        action = env.action_space.sample()
        next_state, reward, done, truncated, _ = env.step(action)
        target = reward + 0.95 * tf.reduce_max(model_reinforcement.predict(next_state.reshape(1, -1)))
        target_f = model_reinforcement.predict(state.reshape(1, -1))
        target_f[0][action] = target
        model_reinforcement.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)
        state = next_state


    # Condição de parada
    if episode % 10 == 0:
        average_reward = sum(reward for _ in range(10)) / 10.0
        print(f'Episode {episode}, Average Reward: {average_reward}')

        # Adicionando uma condição de parada
        if average_reward == 1:  # Pode ajustar esse valor conforme necessário
            print(f'Solved after {episode} episodes!')
            break
